# -*- coding: utf-8 -*-
"""Alzheimers.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1x9cMryphQefdl-O2EduVszz3L0qI6g4S
"""

!pip install roboflow tensorflow tqdm scikit-image

!pip install roboflow

from roboflow import Roboflow
rf = Roboflow(api_key="65bmeBtxW6PMrAoK2coG")
project = rf.workspace("traffic-violation-2hmbm").project("alzheimers-dataset-oasis")
version = project.version(1)
dataset = version.download("multiclass")

import tensorflow as tf
import pandas as pd
import os
from tensorflow.keras.preprocessing.image import load_img, img_to_array
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.applications import EfficientNetB3
from tensorflow.keras.preprocessing.image import ImageDataGenerator
import matplotlib.pyplot as plt
import numpy as np

# Enable mixed precision for reduced memory usage and faster computations
tf.keras.mixed_precision.set_global_policy('mixed_float16')

# Load CSV files
train_df = pd.read_csv('/content/Alzheimers-dataset-oasis-1/train/_classes.csv')
valid_df = pd.read_csv('/content/Alzheimers-dataset-oasis-1/valid/_classes.csv')
test_df = pd.read_csv('/content/Alzheimers-dataset-oasis-1/test/_classes.csv')

# Define separate base paths for each dataset
train_base_path = "/content/Alzheimers-dataset-oasis-1/train"
valid_base_path = "/content/Alzheimers-dataset-oasis-1/valid"
test_base_path = "/content/Alzheimers-dataset-oasis-1/test"

# Convert image filenames to full paths for each dataset
train_df['filename'] = train_df['filename'].apply(lambda x: os.path.join(train_base_path, x))
valid_df['filename'] = valid_df['filename'].apply(lambda x: os.path.join(valid_base_path, x))
test_df['filename'] = test_df['filename'].apply(lambda x: os.path.join(test_base_path, x))

# Preprocessing function
def preprocess_data(image_path):
    img = tf.io.read_file(image_path)
    img = tf.image.decode_jpeg(img, channels=3)
    img = tf.image.resize(img, (224, 224))  # Resize to EfficientNetB3 input size
    img = img / 255.0  # Normalize image
    return img

# Load and preprocess data
def load_and_preprocess_data(df):
    file_paths = df['filename'].values
    labels = df.iloc[:, 1:].values  # Assuming label columns start from index 1
    image_ds = tf.data.Dataset.from_tensor_slices(file_paths)
    image_ds = image_ds.map(lambda x: preprocess_data(x), num_parallel_calls=tf.data.AUTOTUNE)
    label_ds = tf.data.Dataset.from_tensor_slices(labels)
    return tf.data.Dataset.zip((image_ds, label_ds))

# Optimized data pipeline with data augmentation
batch_size = 32  # Adjust batch size based on your GPU memory
train_dataset = load_and_preprocess_data(train_df).batch(batch_size).prefetch(tf.data.AUTOTUNE)
valid_dataset = load_and_preprocess_data(valid_df).batch(batch_size).prefetch(tf.data.AUTOTUNE)
test_dataset = load_and_preprocess_data(test_df).batch(batch_size).prefetch(tf.data.AUTOTUNE)

# Data augmentation generator for the training set
datagen = ImageDataGenerator(
    rotation_range=30,
    width_shift_range=0.2,
    height_shift_range=0.2,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True,
    fill_mode='nearest'
)

# Load EfficientNetB3 as base model
base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(224, 224, 3))

# Freeze the base model layers initially for transfer learning
base_model.trainable = False

# Create a custom model on top of EfficientNetB3
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)  # Adding Dropout to avoid overfitting
output = Dense(4, activation='softmax', dtype='float32')(x)  # 4 classes for Alzheimer's detection

# Build the model
model = Model(inputs=base_model.input, outputs=output)

# Compile the model with Adam optimizer and categorical crossentropy loss
model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Callbacks for early stopping, saving the best model, and reducing learning rate on plateau
checkpoint = ModelCheckpoint('best_model.keras', save_best_only=True, monitor='val_loss', mode='min', verbose=1)
early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True, verbose=1)
lr_scheduler = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=3, verbose=1)

# Train the model with data augmentation and fine-tuning
history = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    epochs=15,
    callbacks=[checkpoint, early_stopping, lr_scheduler]
)

# Unfreeze the top layers of the base model after initial training
base_model.trainable = True
for layer in base_model.layers[:100]:  # Unfreeze the last 100 layers
    layer.trainable = False

# Recompile the model after unfreezing the base layers
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model again after unfreezing base layers
history_fine_tune = model.fit(
    train_dataset,
    validation_data=valid_dataset,
    epochs=10,
    callbacks=[checkpoint, early_stopping, lr_scheduler]
)

# Evaluate the model on the test set
test_loss, test_acc = model.evaluate(test_dataset)
print(f"Test Loss: {test_loss}, Test Accuracy: {test_acc}")

# Predict on a new test image (example: Non-Dementia)
img = load_img('/content/Alzheimers-dataset-oasis-1/test/OAS1_0028_MR1_mpr-1_110_jpg.rf.d094725653dd7403534f44837fd3a747.jpg', target_size=(224, 224))
#img = load_img('/content/Alzheimers-dataset-oasis-1/test/NonDemented/26 (100).jpg', target_size=(224, 224))
img = img_to_array(img)
img = img / 255.0  # Apply the same preprocessing as training
img = np.expand_dims(img, axis=0)  # Add batch dimension

# Predict the class
predictions = model.predict(img)
predicted_class = np.argmax(predictions, axis=1)
probability = round(np.max(predictions) * 100, 2)

# Extract class indices manually
class_indices = {0: 'Mild_Dementia', 1: 'Moderate_Dementia', 2: 'Non_Demented', 3: 'Very_Mild_Dementia'}  # Update this dictionary as per your data
predicted_label = class_indices[predicted_class[0]]

# Print the result
print(f"Probability: {probability}%")
print(f"Predicted Class: {predicted_label}")

# Plot training and validation accuracy/loss graphs
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history_fine_tune.history['accuracy'], label='Train Accuracy')
plt.plot(history_fine_tune.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history_fine_tune.history['loss'], label='Train Loss')
plt.plot(history_fine_tune.history['val_loss'], label='Validation Loss')
plt.title('Loss over epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Predict on a new test image (example: Non-Dementia)
img = load_img('/content/OAS1_0028_MR1_mpr-1_100.jpg', target_size=(224, 224))
#img = load_img('/content/Alzheimers-dataset-oasis-1/test/NonDemented/26 (100).jpg', target_size=(224, 224))
img = img_to_array(img)
img = img / 255.0  # Apply the same preprocessing as training
img = np.expand_dims(img, axis=0)  # Add batch dimension

# Predict the class
predictions = model.predict(img)
predicted_class = np.argmax(predictions, axis=1)
probability = round(np.max(predictions) * 100, 2)

# Extract class indices manually
class_indices = {0: 'Mild_Dementia', 1: 'Moderate_Dementia', 2: 'Non_Demented', 3: 'Very_Mild_Dementia'}  # Update this dictionary as per your data
predicted_label = class_indices[predicted_class[0]]

# Print the result
print(f"Probability: {probability}%")
print(f"Predicted Class: {predicted_label}")

# Plot training and validation accuracy/loss graphs
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history_fine_tune.history['accuracy'], label='Train Accuracy')
plt.plot(history_fine_tune.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history_fine_tune.history['loss'], label='Train Loss')
plt.plot(history_fine_tune.history['val_loss'], label='Validation Loss')
plt.title('Loss over epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()

# Predict on a new test image (example: Non-Dementia)
img = load_img('/content/OAS1_0308_MR1_mpr-1_100.jpg', target_size=(224, 224))
#img = load_img('/content/Alzheimers-dataset-oasis-1/test/NonDemented/26 (100).jpg', target_size=(224, 224))
img = img_to_array(img)
img = img / 255.0  # Apply the same preprocessing as training
img = np.expand_dims(img, axis=0)  # Add batch dimension

# Predict the class
predictions = model.predict(img)
predicted_class = np.argmax(predictions, axis=1)
probability = round(np.max(predictions) * 100, 2)

# Extract class indices manually
class_indices = {0: 'Mild_Dementia', 1: 'Moderate_Dementia', 2: 'Non_Demented', 3: 'Very_Mild_Dementia'}  # Update this dictionary as per your data
predicted_label = class_indices[predicted_class[0]]

# Print the result
print(f"Probability: {probability}%")
print(f"Predicted Class: {predicted_label}")

# Plot training and validation accuracy/loss graphs
plt.figure(figsize=(12, 4))

# Accuracy plot
plt.subplot(1, 2, 1)
plt.plot(history_fine_tune.history['accuracy'], label='Train Accuracy')
plt.plot(history_fine_tune.history['val_accuracy'], label='Validation Accuracy')
plt.title('Accuracy over epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss plot
plt.subplot(1, 2, 2)
plt.plot(history_fine_tune.history['loss'], label='Train Loss')
plt.plot(history_fine_tune.history['val_loss'], label='Validation Loss')
plt.title('Loss over epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.show()